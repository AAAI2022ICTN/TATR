{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML2DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert XML data to DataFrame Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#parameter\n",
    "dataset = 'twitter'\n",
    "base_path = '/data/pengyu/tag_rec/%s/'%dataset\n",
    "data_path = '/data/pengyu/tag_rec/%s/Posts.xml'%dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = 'twitter'\n",
    "vocab_size = 30000\n",
    "#tag_size = \n",
    "bow_vocab = 10000\n",
    "max_src_len = 50\n",
    "max_trg_len = 5\n",
    "\n",
    "base_path = '/data/pengyu/tag_rec/%s/'%dataset\n",
    "\n",
    "train_src = base_path + 'train_src.txt'\n",
    "train_trg = base_path + 'train_trg.txt'\n",
    "valid_src = base_path + 'valid_src.txt'\n",
    "valid_trg = base_path + 'valid_trg.txt'\n",
    "test_src = base_path + 'test_src.txt'\n",
    "test_trg = base_path + 'test_trg.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading 35290 lines of data from /data/pengyu/tag_rec/twitter/train_src.txt and /data/pengyu/tag_rec/twitter/train_trg.txt\n"
     ]
    }
   ],
   "source": [
    "def read_src_trg_files(tag=\"train\"):\n",
    "    '''\n",
    "    Read data according to the tag (train/valid/test), return a list of (src, trg) pairs\n",
    "    '''\n",
    "    if tag == \"train\":\n",
    "        src_file = train_src\n",
    "        trg_file = train_trg\n",
    "    elif tag == \"valid\":\n",
    "        src_file = valid_src\n",
    "        trg_file = valid_trg\n",
    "    else:\n",
    "        assert tag == \"test\"\n",
    "        src_file = test_src\n",
    "        trg_file = test_trg\n",
    "\n",
    "    tokenized_src = []\n",
    "    tokenized_trg = []\n",
    "\n",
    "    for src_line, trg_line in zip(open(src_file, 'r'), open(trg_file, 'r')):\n",
    "        # process src and trg line\n",
    "        src_word_list = src_line.strip().split(' ')\n",
    "        trg_list = trg_line.strip().split(';')  # a list of target sequences\n",
    "        #trg_word_list = [trg.strip().split(' ') for trg in trg_list]\n",
    "\n",
    "        # Truncate the sequence if it is too long\n",
    "        src_word_list = src_word_list[:max_src_len]\n",
    "        trg_word_list = trg_list[:max_trg_len]\n",
    "\n",
    "        # Append the lines to the data\n",
    "        tokenized_src.append(src_word_list)\n",
    "        tokenized_trg.append(trg_word_list)\n",
    "\n",
    "    assert len(tokenized_src) == len(tokenized_trg), \\\n",
    "        'the number of records in source and target are not the same'\n",
    "\n",
    "    tokenized_pairs = list(zip(tokenized_src, tokenized_trg))\n",
    "    print(\"Finish reading %d lines of data from %s and %s\" % (len(tokenized_src), src_file, trg_file))\n",
    "    return tokenized_pairs\n",
    "\n",
    "# Tokenize training data, return a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])\n",
    "tokenized_train_pairs = read_src_trg_files(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary from training data\n",
      "Total vocab_size: 29712, predefined vocab_size: 30000\n",
      "Total tag_size: 3671\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_src_trg_pairs):\n",
    "    '''\n",
    "    Build the vocabulary from the training (src, trg) pairs\n",
    "    :param tokenized_src_trg_pairs: list of (src, trg) pairs\n",
    "    :return: word2idx, idx2word, token_freq_counter\n",
    "    '''\n",
    "    token_freq_counter = Counter()\n",
    "    token_freq_counter_tag = Counter()\n",
    "    for src_word_list, trg_word_lists in tokenized_src_trg_pairs:\n",
    "        token_freq_counter.update(src_word_list)\n",
    "        token_freq_counter_tag.update(trg_word_lists)\n",
    "\n",
    "    # Discard special tokens if already present\n",
    "    special_tokens = ['<pad>', '<unk>']\n",
    "    num_special_tokens = len(special_tokens)\n",
    "\n",
    "    for s_t in special_tokens:\n",
    "        if s_t in token_freq_counter:\n",
    "            del token_freq_counter[s_t]\n",
    "\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for idx, word in enumerate(special_tokens):\n",
    "        # '<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    sorted_word2idx = sorted(token_freq_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_words = [x[0] for x in sorted_word2idx]\n",
    "\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        word2idx[word] = idx + num_special_tokens\n",
    "\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        idx2word[idx + num_special_tokens] = word\n",
    "\n",
    "    tag2idx = dict()\n",
    "    idx2tag = dict()\n",
    "\n",
    "    sorted_tag2idx = sorted(token_freq_counter_tag.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_tags = [x[0] for x in sorted_tag2idx]\n",
    "\n",
    "    for idx, tag in enumerate(sorted_tags):\n",
    "        tag2idx[tag] = idx\n",
    "\n",
    "    for idx, tag in enumerate(sorted_tags):\n",
    "        idx2tag[idx] = tag        \n",
    "             \n",
    "    return word2idx, idx2word, token_freq_counter, tag2idx, idx2tag\n",
    "\n",
    "# Build vocabulary from training src and trg\n",
    "print(\"Building vocabulary from training data\")\n",
    "word2idx, idx2word, token_freq_counter, tag2idx, idx2tag = build_vocab(tokenized_train_pairs)\n",
    "print(\"Total vocab_size: %d, predefined vocab_size: %d\" % (len(word2idx), vocab_size))\n",
    "print(\"Total tag_size: %d\" %len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bow dictionary from training data\n",
      "The original bow vocabulary: 32195\n",
      "The top 50 non-stop-words:  [('jan 25', 3993), ('egypt', 3891), ('<number>', 3881), ('like', 2480), ('people', 1957), ('super bowl', 1738), ('love', 1619), ('know', 1359), ('mubarak', 1308), ('time', 1225), ('good', 1205), ('want', 1126), ('lol', 1096), ('sotu', 1060), ('think', 1053), ('got', 1044), ('tahrir', 1026), ('day', 1005), ('super', 974), ('bowl', 915), ('life', 854), ('going', 845), ('need', 839), ('quote', 820), ('right', 815), ('today', 768), ('<happy>', 749), ('man', 706), ('game', 687), ('shit', 681), ('now playing', 661), ('im', 651), ('way', 650), ('just saying', 645), ('fail', 635), ('black', 633), ('let', 624), ('hate', 616), ('twitter', 570), ('steelers', 567), ('come', 555), ('world', 550), ('random', 544), ('new', 541), ('dont', 540), ('feel', 532), ('playing', 529), ('best', 523), ('oh', 523), ('girl', 520), ('followers', 516), ('packers', 509), ('better', 502), ('look', 498), ('watching', 497), ('cairo', 496), ('work', 490), ('protesters', 489), ('said', 485), ('watch', 475), ('stop', 474), ('egyptian', 473), ('gonna', 465), ('wish', 461), ('wanna', 459), ('night', 458), ('fact', 448), ('live', 443), ('tell', 442), ('damn', 434), ('fuck', 430), ('ass', 429), ('says', 427), ('things', 426), ('square', 423), ('getting', 421), ('thing', 416), ('team', 402), ('police', 401), ('god', 399), ('real', 395), ('obama', 393), ('ppl', 390), ('big', 385), ('tonight', 376), ('bad', 372), ('phone', 369), ('home', 367), ('al', 366), ('ur', 366), ('person', 365), ('talk', 362), ('<time>', 357), ('one of my followers', 354), ('friends', 350), ('lmao', 349), ('great', 344), ('year', 342), ('wait', 339), ('glee', 339), ('tv', 332), ('girls', 329), ('quotes', 329), ('long', 328), ('rt', 324), ('hope', 321), ('tweet', 317), ('away', 316), ('tomorrow', 315), ('army', 312), ('rhoa', 309), ('win', 308), ('heart', 307), ('head', 305), ('trying', 303), ('real talk', 302), ('face', 300), ('old', 297), ('school', 297), ('sleep', 295), ('are the worst', 295), ('start', 295), ('little', 294), ('care', 292), ('thought', 290), ('guy', 290), ('end', 289), ('days', 289), ('saying', 286), ('state', 285), ('cause', 282), ('help', 282), ('bitch', 281), ('looks', 281), ('moment', 279), ('news', 277), ('tweets', 276), ('hell', 274), ('leave', 273), ('mean', 273), ('happy', 271), ('money', 271), ('change', 270), ('play', 268), ('jan', 267), ('guys', 266), ('house', 266), ('dead', 266), ('white', 265), ('oh teen quotes', 264), ('jazeera', 263), ('stay', 263), ('ask', 262), ('try', 262), ('25', 261), ('sure', 261), ('hard', 259), ('smh', 258), ('follow', 257), ('friend', 253), ('half', 253), ('remember', 250), ('bout', 248), ('oomf', 248), ('morning', 247), ('looking', 246), ('use', 246), ('song', 246), ('ok', 245), ('wrong', 245), ('wit', 244), ('actually', 244), ('mind', 244), ('believe', 243), ('da', 242), ('<sad>', 241), ('years', 241), ('seen', 241), ('president', 240), ('makes', 240), ('aint', 239), ('egyptians', 238), ('woman', 238), ('nice', 238), ('hair', 237), ('subtweet', 237), ('dear', 236), ('left', 236), ('truth', 235), ('having', 235), ('told', 233), ('green', 232), ('team follow back', 231), ('i cant date you', 231), ('government', 229), ('omg that is a teen', 229), ('talking', 228), ('facebook', 225), ('gotta', 225), ('bed', 225)]\n",
      "Bow dict_size: 10000 after filtered\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def make_bow_dictionary(tokenized_src_trg_pairs, base_path, bow_vocab):\n",
    "    '''\n",
    "    Build bag-of-word dictionary from tokenized_src_trg_pairs\n",
    "    :param tokenized_src_trg_pairs: a list of (src, trg) pairs\n",
    "    :param data_dir: data address, for distinguishing Weibo/Twitter/StackExchange\n",
    "    :param bow_vocab: the size the bow vocabulary\n",
    "    :return: bow_dictionary, a gensim.corpora.Dictionary object\n",
    "    '''\n",
    "    doc_bow = []\n",
    "    tgt_set = set()\n",
    "\n",
    "    for src, tgt in tokenized_src_trg_pairs:\n",
    "        cur_bow = []\n",
    "        cur_bow.extend(src)\n",
    "        cur_bow.extend(tgt)\n",
    "        doc_bow.append(cur_bow)\n",
    "        \n",
    "    bow_dictionary = gensim.corpora.Dictionary(doc_bow)\n",
    "    # Remove single letter or character tokens\n",
    "    len_1_words = list(filter(lambda w: len(w) == 1, bow_dictionary.values()))\n",
    "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, len_1_words)))\n",
    "\n",
    "    def read_stopwords(fn):\n",
    "        return set([line.strip() for line in open(fn, encoding='utf-8') if len(line.strip()) != 0])\n",
    "\n",
    "    # Read stopwords from file (bow vocabulary should not contain stopwords)\n",
    "    STOPWORDS = gensim.parsing.preprocessing.STOPWORDS\n",
    "    #stopwords1 = read_stopwords(base_path+\"stopwords/stopwords.en.txt\")\n",
    "    #stopwords2 = read_stopwords(base_path+\"stopwords/stopwords.SE.txt\")\n",
    "    final_stopwords = set(STOPWORDS)#.union(stopwords1).union(stopwords2)\n",
    "\n",
    "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, final_stopwords)))\n",
    "\n",
    "    print(\"The original bow vocabulary: %d\" % len(bow_dictionary))\n",
    "    bow_dictionary.filter_extremes(no_below=3, keep_n=bow_vocab)\n",
    "    bow_dictionary.compactify()\n",
    "    bow_dictionary.id2token = dict([(id, t) for t, id in bow_dictionary.token2id.items()])\n",
    "    # for debug\n",
    "    sorted_dfs = sorted(bow_dictionary.dfs.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_dfs_token = [(bow_dictionary.id2token[id], cnt) for id, cnt in sorted_dfs]\n",
    "    print('The top 50 non-stop-words: ', sorted_dfs_token[:200])\n",
    "    return bow_dictionary\n",
    "\n",
    "# Build bag-of-word dictionary from training data\n",
    "print(\"Building bow dictionary from training data\")\n",
    "bow_dictionary = make_bow_dictionary(tokenized_train_pairs, base_path, bow_vocab)\n",
    "print(\"Bow dict_size: %d after filtered\" % len(bow_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict word2idx Saved\n",
      "Dict idx2word Saved\n",
      "Dict tag2idx Saved\n",
      "Dict idx2tag Saved\n",
      "Dict token_freq_counter Saved\n",
      "Dict bow_dictionary Saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(mydict, dict_name):\n",
    "    file_name=base_path+dict_name\n",
    "    f=open(file_name,'wb')\n",
    "    pickle.dump(mydict,f)\n",
    "    f.close()\n",
    "    print('Dict %s Saved'%dict_name)\n",
    "    \n",
    "save_dict(word2idx, 'word2idx')\n",
    "save_dict(idx2word, 'idx2word')\n",
    "save_dict(tag2idx, 'tag2idx')\n",
    "save_dict(idx2tag, 'idx2tag')\n",
    "save_dict(token_freq_counter, 'token_freq_counter')\n",
    "save_dict(bow_dictionary, 'bow_dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocab_size = 30000\n",
    "#tag_size = \n",
    "bow_vocab = 10000\n",
    "max_src_len = 50\n",
    "max_trg_len = 5\n",
    "\n",
    "def load_dict(dict_name):\n",
    "    file_name=base_path+dict_name\n",
    "    f=open(file_name,'rb')\n",
    "    mydict=pickle.load(f)\n",
    "    f.close()\n",
    "    return mydict\n",
    "\n",
    "bow_dictionary = load_dict('bow_dictionary')\n",
    "word2idx = load_dict('word2idx')\n",
    "tag2idx = load_dict('tag2idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_src_trg_files(tag=\"train\"):\n",
    "    '''\n",
    "    Read data according to the tag (train/valid/test), return a list of (src, trg) pairs\n",
    "    '''\n",
    "    if tag == \"train\":\n",
    "        src_file = train_src\n",
    "        trg_file = train_trg\n",
    "    elif tag == \"valid\":\n",
    "        src_file = valid_src\n",
    "        trg_file = valid_trg\n",
    "    else:\n",
    "        assert tag == \"test\"\n",
    "        src_file = test_src\n",
    "        trg_file = test_trg\n",
    "\n",
    "    tokenized_src = []\n",
    "    tokenized_trg = []\n",
    "    avg_post = []\n",
    "    avg_tag = []\n",
    "\n",
    "    for src_line, trg_line in zip(open(src_file, 'r'), open(trg_file, 'r')):\n",
    "        # process src and trg line\n",
    "        src_word_list = src_line.strip().split(' ')\n",
    "        trg_list = trg_line.strip().split(';')  # a list of target sequences\n",
    "\n",
    "        # Truncate the sequence if it is too long\n",
    "        avg_post.append(len(src_word_list))\n",
    "        avg_tag.append(len(trg_list))\n",
    "        src_word_list = src_word_list[:max_src_len]\n",
    "        trg_word_list = trg_list[:max_trg_len]\n",
    "\n",
    "        # Append the lines to the data\n",
    "        tokenized_src.append(src_word_list)\n",
    "        tokenized_trg.append(trg_word_list)\n",
    "\n",
    "    assert len(tokenized_src) == len(tokenized_trg), \\\n",
    "        'the number of records in source and target are not the same'\n",
    "\n",
    "    tokenized_pairs = list(zip(tokenized_src, tokenized_trg))\n",
    "    print(\"Finish reading %d lines of data from %s and %s\" % (len(tokenized_src), src_file, trg_file))\n",
    "    print('avg_post',np.mean(avg_post))\n",
    "    print('avg_tag',np.mean(avg_tag))\n",
    "    return tokenized_pairs\n",
    "\n",
    "def build_dataset(src_trgs_pairs, word2idx, tag2idx, bow_dictionary, tag=\"train\"):\n",
    "    '''\n",
    "    build train/valid/test dataset\n",
    "    '''\n",
    "    text = []\n",
    "    label = []\n",
    "    bow = [] \n",
    "    for idx, (source, targets) in enumerate(src_trgs_pairs):\n",
    "        src = [word2idx[w] if w in word2idx and word2idx[w] < vocab_size\n",
    "               else word2idx['<unk>'] for w in source]\n",
    "        trg = [tag2idx[w] for w in targets if w in tag2idx]\n",
    "        src_bow = bow_dictionary.doc2bow(source)\n",
    "        text.append(src)\n",
    "        label.append(trg)\n",
    "        bow.append(src_bow)\n",
    "        \n",
    "    bow = BowFeature(bow, bow_dictionary)\n",
    "    text = padding(text)\n",
    "    label =  [encode_one_hot(inst, len(tag2idx), label_from=0) for inst in label] \n",
    "    return np.array(text), np.array(label), np.array(bow)\n",
    "\n",
    "def padding(input_list):\n",
    "    input_list_lens = [len(l) for l in input_list]\n",
    "    max_seq_len = max(max(input_list_lens),max_src_len)\n",
    "    padded_batch = word2idx['<pad>'] * np.ones((len(input_list), max_seq_len), dtype=np.int)\n",
    "\n",
    "    for j in range(len(input_list)):\n",
    "        current_len = input_list_lens[j]\n",
    "        padded_batch[j][:current_len] = input_list[j]\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "def BowFeature(input_list, bow_dictionary):\n",
    "    '''\n",
    "    generate Bow Feature for train\\val\\test src\n",
    "    '''\n",
    "    bow_vocab = len(bow_dictionary)\n",
    "    res_src_bow = np.zeros((len(input_list), bow_vocab), dtype=np.int)\n",
    "    for idx, bow in enumerate(input_list):\n",
    "        bow_k = [k for k, v in bow]\n",
    "        bow_v = [v for k, v in bow]\n",
    "        res_src_bow[idx, bow_k] = bow_v\n",
    "    return res_src_bow\n",
    "\n",
    "def encode_one_hot(inst, vocab_size, label_from):\n",
    "    '''\n",
    "    one hot for a value x, int, x>=1\n",
    "    '''\n",
    "    one_hots = np.zeros(vocab_size, dtype=np.float32)\n",
    "    for value in inst:\n",
    "        one_hots[value-label_from]=1\n",
    "    return one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading 35290 lines of data from /data/pengyu/tag_rec/twitter/train_src.txt and /data/pengyu/tag_rec/twitter/train_trg.txt\n",
      "avg_post 19.50385378294134\n",
      "avg_tag 1.128534995749504\n",
      "Finish reading 4411 lines of data from /data/pengyu/tag_rec/twitter/valid_src.txt and /data/pengyu/tag_rec/twitter/valid_trg.txt\n",
      "avg_post 19.62117433688506\n",
      "avg_tag 1.1308093402856496\n",
      "Finish reading 4412 lines of data from /data/pengyu/tag_rec/twitter/test_src.txt and /data/pengyu/tag_rec/twitter/test_trg.txt\n",
      "avg_post 19.57683590208522\n",
      "avg_tag 1.1260199456029012\n"
     ]
    }
   ],
   "source": [
    "# Build training set\n",
    "tokenized_train_pairs = read_src_trg_files(\"train\")\n",
    "train_text, train_label, train_bow = build_dataset(tokenized_train_pairs, word2idx, tag2idx, bow_dictionary,\"train\")\n",
    "\n",
    "tokenized_valid_pairs = read_src_trg_files('valid')\n",
    "valid_text, valid_label, valid_bow = build_dataset(tokenized_valid_pairs, word2idx, tag2idx, bow_dictionary,\"valid\")\n",
    "\n",
    "tokenized_test_pairs = read_src_trg_files('test')\n",
    "test_text, test_label, test_bow = build_dataset(tokenized_test_pairs, word2idx, tag2idx, bow_dictionary,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/pengyu/tag_rec/twitter/processed_data/train_text.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/train_label.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/train_bow.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/valid_text.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/valid_label.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/valid_bow.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/test_text.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/test_label.npy Saved\n",
      "/data/pengyu/tag_rec/twitter/processed_data/test_bow.npy Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_data(name):\n",
    "    data = eval(name)\n",
    "    \n",
    "    print('%s saved, shape:'%name, data.shape)\n",
    "    \n",
    "def save_data(data, name):\n",
    "    path = save_path+'/%s.npy'%name\n",
    "    np.save(path, data, allow_pickle=True) \n",
    "    print('%s Saved'%path)\n",
    "\n",
    "save_path = base_path+ 'processed_data'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "save_data(train_text, 'train_text')\n",
    "save_data(train_label, 'train_label')\n",
    "save_data(train_bow, 'train_bow')\n",
    "save_data(valid_text, 'valid_text')\n",
    "save_data(valid_label, 'valid_label')\n",
    "save_data(valid_bow, 'valid_bow')\n",
    "save_data(test_text, 'test_text')\n",
    "save_data(test_label, 'test_label')\n",
    "save_data(test_bow, 'test_bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-f50cb13abe2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#BoW feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_bow_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_bow_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_bow_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_utils' is not defined"
     ]
    }
   ],
   "source": [
    "#BoW feature\n",
    "train_bow_data = data_utils.TensorDataset(torch.from_numpy(train_bow).type(torch.float32))\n",
    "val_bow_data = data_utils.TensorDataset(torch.from_numpy(valid_bow).type(torch.float32))                                          \n",
    "test_bow_data = data_utils.TensorDataset(torch.from_numpy(test_bow).type(torch.float32))\n",
    "\n",
    "train_bow_loader = data_utils.DataLoader(train_bow_data, batch_size, shuffle=True, drop_last=True)\n",
    "valid_bow_loader = data_utils.DataLoader(val_bow_data, batch_size, shuffle=True, drop_last=True)\n",
    "test_bow_loader = data_utils.DataLoader(test_bow_data, batch_size, drop_last=True)\n",
    "\n",
    "#Nomral feature and label\n",
    "train_data = data_utils.TensorDataset(torch.from_numpy(train_bow).type(torch.float32),\n",
    "                                      torch.from_numpy(train_text).type(torch.LongTensor),\n",
    "                                      torch.from_numpy(train_label).type(torch.LongTensor))\n",
    "val_data = data_utils.TensorDataset(torch.from_numpy(valid_bow).type(torch.float32),\n",
    "                                    torch.from_numpy(valid_text).type(torch.LongTensor),\n",
    "                                      torch.from_numpy(valid_label).type(torch.LongTensor))                                          \n",
    "test_data = data_utils.TensorDataset(torch.from_numpy(test_bow).type(torch.float32),\n",
    "                                     torch.from_numpy(test_text).type(torch.LongTensor),\n",
    "                                     torch.from_numpy(test_label).type(torch.LongTensor))\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size, drop_last=True)\n",
    "\n",
    "label_num = int(train_label.max())\n",
    "vocab_size = int(train_text.max())+2# +2 Don't Know Why\n",
    "fp('label_num')\n",
    "fp('vocab_size')\n",
    "print(\"load done\")\n",
    "\n",
    "return train_loader, val_loader, test_loader, label_num, vocab_size, train_bow_loader, \\\n",
    "valid_bow_loader, test_bow_loader, bow_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
