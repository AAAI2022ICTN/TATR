{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML2DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Number： 188296\n",
      "original data Number： 345070\n"
     ]
    }
   ],
   "source": [
    "#Convert XML data to DataFrame Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#parameter\n",
    "dataset = 'AU'\n",
    "base_path = '/data/pengyu/tag_rec/%s/'%dataset\n",
    "data_path = '/data/pengyu/tag_rec/%s/Posts.xml'%dataset\n",
    "\n",
    "def xml2df(data_path):\n",
    "    '''\n",
    "    Input: XML data path\n",
    "    Output: Dataframe data that we need\n",
    "    '''\n",
    "    tree = ET.ElementTree(file=data_path)\n",
    "    root = tree.getroot()\n",
    "    body=[]\n",
    "    title=[]\n",
    "    tag=[]\n",
    "    userid=[]\n",
    "    for child_of_root in root:\n",
    "#         print(child_of_root.attrib)\n",
    "#         break\n",
    "        title.append(np.nan if child_of_root.get('Title')==None else child_of_root.attrib['Title'])\n",
    "        body.append(np.nan if child_of_root.get('Body')==None else child_of_root.attrib['Body'])\n",
    "        tag.append(np.nan if child_of_root.get('Tags')==None else child_of_root.attrib['Tags'])\n",
    "        userid.append(np.nan if child_of_root.get('OwnerUserId')==None else child_of_root.attrib['OwnerUserId'])\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "          'title':title,\n",
    "          'body':body,\n",
    "          'tag':tag,\n",
    "          'userid':userid\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = xml2df(data_path)\n",
    "df=df.dropna()\n",
    "df['body'] = df['title']+df['body']\n",
    "user_num = len(df['userid'].drop_duplicates())\n",
    "print('User Number：', user_num)\n",
    "df = df[['body','tag']]\n",
    "df = shuffle(df, random_state=42)\n",
    "df = df.iloc[:,:]\n",
    "print('original data Number：', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Html Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length is 345070\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "wnl = WordNetLemmatizer() \n",
    "sw=stopwords.words('english')\n",
    "\n",
    "def src_remove_html(sentence):\n",
    "    temp = sentence\n",
    "    temp = re.sub(r'<[^>]+>', ' ',str(temp))#delete <*>\n",
    "    temp = re.sub(r'\\n', ' ',str(temp))#delete \\n\n",
    "    temp = re.sub(r'\\ \\d+\\ ', ' ',str(temp))#remove int number\n",
    "    temp = re.sub(r'\\ \\d+\\.\\d+\\ ', ' ',str(temp))#remove float number\n",
    "    temp = re.sub(r'\\s+[a-zA-Z]\\s+', ' ',str(temp))#remove singel letter  \n",
    "    temp = re.sub(r'[\\.,!?]\\s', ' ',str(temp))#remove , .for sentence \n",
    "    temp = re.sub(r'\\s[\\(\\[]|[\\)\\}]\\s', ' ',str(temp))#remove ()[]for sentence \n",
    "    temp = re.sub(r'\\:\\s', ' ',str(temp))#remove :for sentence \n",
    "    temp = re.sub(r'\\s+', ' ',str(temp))#merge many ' '\n",
    "    temp = temp.lower()\n",
    "    doc = temp.split()\n",
    "    temp = [wnl.lemmatize(word) for word in doc if word not in sw]\n",
    "    temp = ' '.join(temp)   \n",
    "    return temp\n",
    "\n",
    "def trg_remove_symbol(sentence):\n",
    "    temp = sentence\n",
    "    #temp = re.sub(r'[<>]', ' ',str(temp))\n",
    "    temp = re.sub(r'><', ';',str(temp))\n",
    "    temp = re.sub(r'[<>]', '',str(temp))\n",
    "    return temp    \n",
    "\n",
    "src = []\n",
    "trg = []\n",
    "\n",
    "for src_line, trg_line in df.values:\n",
    "    src_line = src_remove_html(src_line)    \n",
    "    trg_line = trg_remove_symbol(trg_line)\n",
    "    #delete Na row\n",
    "    if len(src_line)<2 or len(trg_line)<1:\n",
    "        print('=========================================')\n",
    "        print(src_line)\n",
    "        print(trg_line)\n",
    "        continue\n",
    "    src.append(src_line)\n",
    "    trg.append(trg_line)\n",
    "    \n",
    "assert len(src) == len(trg), \\\n",
    "    'the number of records in source and target are not the same'    \n",
    "print('data length is %d'%len(trg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(src)\n",
    "train_length = int(data_length*.8)\n",
    "valid_length = int(data_length*.9)\n",
    "train_src, valid_src, test_src = src[:train_length], src[train_length:valid_length],\\\n",
    "                                 src[valid_length:]\n",
    "train_trg, valid_trg, test_trg = trg[:train_length], trg[train_length:valid_length], \\\n",
    "                                 trg[valid_length:]\n",
    "\n",
    "# i = 177\n",
    "# print(df.iloc[i,0])\n",
    "# print(src[i])\n",
    "# print(tokenized_src[i])\n",
    "# print(trg[i])\n",
    "# print(tokenized_trg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_src Saved\n",
      "valid_src Saved\n",
      "test_src Saved\n",
      "train_trg Saved\n",
      "valid_trg Saved\n",
      "test_trg Saved\n"
     ]
    }
   ],
   "source": [
    "def save_data(data, name):\n",
    "    with open(base_path+'%s.txt'%name, 'w') as f:\n",
    "        f.writelines(\"%s\\n\" % line for line in data)    \n",
    "\n",
    "save_data(train_src, 'train_src')\n",
    "save_data(valid_src, 'valid_src')\n",
    "save_data(test_src, 'test_src')\n",
    "save_data(train_trg, 'train_trg')\n",
    "save_data(valid_trg, 'valid_trg')\n",
    "save_data(test_trg, 'test_trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = 'AU'\n",
    "vocab_size = 50000\n",
    "#tag_size = \n",
    "bow_vocab = 10000\n",
    "max_src_len = 200\n",
    "max_trg_len = 5\n",
    "\n",
    "base_path = '/data/pengyu/tag_rec/%s/'%dataset\n",
    "\n",
    "train_src = base_path + 'train_src.txt'\n",
    "train_trg = base_path + 'train_trg.txt'\n",
    "valid_src = base_path + 'valid_src.txt'\n",
    "valid_trg = base_path + 'valid_trg.txt'\n",
    "test_src = base_path + 'test_src.txt'\n",
    "test_trg = base_path + 'test_trg.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_src_trg_files(tag=\"train\"):\n",
    "    '''\n",
    "    Read data according to the tag (train/valid/test), return a list of (src, trg) pairs\n",
    "    '''\n",
    "    if tag == \"train\":\n",
    "        src_file = train_src\n",
    "        trg_file = train_trg\n",
    "    elif tag == \"valid\":\n",
    "        src_file = valid_src\n",
    "        trg_file = valid_trg\n",
    "    else:\n",
    "        assert tag == \"test\"\n",
    "        src_file = test_src\n",
    "        trg_file = test_trg\n",
    "\n",
    "    tokenized_src = []\n",
    "    tokenized_trg = []\n",
    "\n",
    "    for src_line, trg_line in zip(open(src_file, 'r'), open(trg_file, 'r')):\n",
    "        # process src and trg line\n",
    "        src_word_list = src_line.strip().split(' ')\n",
    "        trg_list = trg_line.strip().split(';')  # a list of target sequences\n",
    "        #trg_word_list = [trg.strip().split(' ') for trg in trg_list]\n",
    "\n",
    "        # Truncate the sequence if it is too long\n",
    "        src_word_list = src_word_list[:max_src_len]\n",
    "        trg_word_list = trg_list[:max_trg_len]\n",
    "\n",
    "        # Append the lines to the data\n",
    "        tokenized_src.append(src_word_list)\n",
    "        tokenized_trg.append(trg_word_list)\n",
    "\n",
    "    assert len(tokenized_src) == len(tokenized_trg), \\\n",
    "        'the number of records in source and target are not the same'\n",
    "\n",
    "    tokenized_pairs = list(zip(tokenized_src, tokenized_trg))\n",
    "    print(\"Finish reading %d lines of data from %s and %s\" % (len(tokenized_src), src_file, trg_file))\n",
    "    return tokenized_pairs\n",
    "\n",
    "# Tokenize training data, return a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])\n",
    "tokenized_train_pairs = read_src_trg_files(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary from training data\n",
      "Total vocab_size: 973836, predefined vocab_size: 50000\n",
      "Total tag_size: 3097\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_src_trg_pairs):\n",
    "    '''\n",
    "    Build the vocabulary from the training (src, trg) pairs\n",
    "    :param tokenized_src_trg_pairs: list of (src, trg) pairs\n",
    "    :return: word2idx, idx2word, token_freq_counter\n",
    "    '''\n",
    "    token_freq_counter = Counter()\n",
    "    token_freq_counter_tag = Counter()\n",
    "    for src_word_list, trg_word_lists in tokenized_src_trg_pairs:\n",
    "        token_freq_counter.update(src_word_list)\n",
    "        token_freq_counter_tag.update(trg_word_lists)\n",
    "\n",
    "    # Discard special tokens if already present\n",
    "    special_tokens = ['<pad>', '<unk>']\n",
    "    num_special_tokens = len(special_tokens)\n",
    "\n",
    "    for s_t in special_tokens:\n",
    "        if s_t in token_freq_counter:\n",
    "            del token_freq_counter[s_t]\n",
    "\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for idx, word in enumerate(special_tokens):\n",
    "        # '<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    sorted_word2idx = sorted(token_freq_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_words = [x[0] for x in sorted_word2idx]\n",
    "\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        word2idx[word] = idx + num_special_tokens\n",
    "\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        idx2word[idx + num_special_tokens] = word\n",
    "\n",
    "    tag2idx = dict()\n",
    "    idx2tag = dict()\n",
    "\n",
    "    sorted_tag2idx = sorted(token_freq_counter_tag.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_tags = [x[0] for x in sorted_tag2idx]\n",
    "\n",
    "    for idx, tag in enumerate(sorted_tags):\n",
    "        tag2idx[tag] = idx\n",
    "\n",
    "    for idx, tag in enumerate(sorted_tags):\n",
    "        idx2tag[idx] = tag        \n",
    "             \n",
    "    return word2idx, idx2word, token_freq_counter, tag2idx, idx2tag\n",
    "\n",
    "# Build vocabulary from training src and trg\n",
    "print(\"Building vocabulary from training data\")\n",
    "word2idx, idx2word, token_freq_counter, tag2idx, idx2tag = build_vocab(tokenized_train_pairs)\n",
    "print(\"Total vocab_size: %d, predefined vocab_size: %d\" % (len(word2idx), vocab_size))\n",
    "print(\"Total tag_size: %d\" %len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bow dictionary from training data\n",
      "The original bow vocabulary: 973454\n",
      "The top 50 non-stop-words:  [('file', 75173), ('install', 70808), ('installed', 70068), ('work', 63683), ('problem', 62960), (\"i'm\", 62415), ('tried', 61891), ('use', 60450), ('like', 57296), ('window', 56747), ('error', 54145), ('want', 49935), ('help', 47880), ('command', 45000), ('running', 44896), ('run', 41993), ('know', 41903), ('sudo', 40590), ('way', 40383), ('boot', 39966), ('trying', 39269), ('new', 39260), ('time', 39116), ('need', 38260), (\"i've\", 38191), ('version', 37105), (\"can't\", 36455), ('following', 34937), ('working', 33334), ('try', 33204), ('server', 30930), ('screen', 30928), ('laptop', 30303), ('update', 29622), ('package', 29619), ('thanks', 29360), ('linux', 29104), ('issue', 29073), ('installation', 28071), ('set', 27155), ('option', 27011), ('change', 26911), ('start', 26652), ('setting', 26453), ('terminal', 25761), ('installing', 25616), ('fix', 25140), ('driver', 24777), ('open', 24472), ('fine', 24209), ('question', 23738), ('able', 23399), ('desktop', 23167), ('device', 23074), ('usb', 22968), ('line', 22226), ('directory', 21911), ('drive', 21888), ('output', 21834), ('possible', 21526), ('got', 21445), ('lts', 21364), ('apt-get', 21210), ('user', 20940), ('14.04', 20850), ('message', 20535), ('thing', 20303), ('machine', 20265), ('default', 19451), ('partition', 19200), ('network', 18777), ('failed', 18773), ('16.04', 18742), ('disk', 18605), ('access', 18300), ('solution', 18125), ('software', 17900), ('idea', 17864), ('12.04', 16946), ('right', 16919), ('unable', 16902), ('kernel', 16399), ('application', 16242), ('upgrade', 15960), ('folder', 15852), ('connect', 15724), ('edit', 15671), ('different', 15645), ('program', 15596), ('configuration', 15497), ('script', 15442), ('connection', 15409), ('command-line', 15391), ('18.04', 15324), ('remove', 15220), ('type', 15217), ('networking', 15144), ('sure', 15068), ('root', 14875), ('getting', 14863), ('apt', 14817), ('log', 14775), ('look', 14448), ('read', 14378), ('card', 14208), ('create', 14065), ('second', 13915), ('add', 13724), ('list', 13718), ('gnome', 13641), ('hard', 13606), ('think', 13606), ('recently', 13540), ('available', 13480), ('intel', 13336), ('process', 13331), ('display', 13262), ('menu', 13153), ('instead', 13113), ('bit', 13076), ('login', 13074), ('wireless', 12925), ('graphic', 12894), ('result', 12873), ('mode', 12845), ('password', 12840), ('information', 12836), ('pc', 12754), ('dual-boot', 12750), ('unity', 12711), ('internet', 12677), ('dual', 12259), ('key', 12227), ('wrong', 12224), ('old', 12120), ('grub', 12029), ('created', 11941), ('currently', 11862), ('worked', 11769), ('nvidia', 11640), ('example', 11636), ('data', 11633), ('code', 11443), ('answer', 11423), ('drivers', 11387), ('going', 11374), ('manager', 11238), ('image', 11125), ('link', 11028), ('state', 10994), ('reboot', 10966), ('click', 10849), ('video', 10744), ('keyboard', 10700), ('space', 10690), ('reading', 10677), ('missing', 10617), ('download', 10470), ('connected', 10393), ('upgraded', 10330), ('lot', 10273), ('source', 10271), ('dependency', 10200), ('info', 10184), ('check', 10163), ('tell', 10152), ('started', 10129), ('mean', 10107), ('setup', 10077), ('size', 9979), ('correct', 9889), ('appreciated', 9888), ('restart', 9781), ('wifi', 9773), ('bash', 9703), ('load', 9687), ('thank', 9687), ('current', 9633), ('partitioning', 9606), (\"i'd\", 9559), ('point', 9498), ('advance', 9473), ('icon', 9468), ('come', 9461), ('home', 9287), ('id', 9256), ('looking', 9238), ('case', 9195), ('solve', 9177), ('mount', 9177), ('monitor', 9167), ('changed', 9152), ('button', 9136), ('day', 9129), ('downloaded', 9125), ('step', 9123), ('grep', 9112), ('happens', 9112), ('support', 9091), ('automatically', 9039)]\n",
      "Bow dict_size: 10000 after filtered\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def make_bow_dictionary(tokenized_src_trg_pairs, base_path, bow_vocab):\n",
    "    '''\n",
    "    Build bag-of-word dictionary from tokenized_src_trg_pairs\n",
    "    :param tokenized_src_trg_pairs: a list of (src, trg) pairs\n",
    "    :param data_dir: data address, for distinguishing Weibo/Twitter/StackExchange\n",
    "    :param bow_vocab: the size the bow vocabulary\n",
    "    :return: bow_dictionary, a gensim.corpora.Dictionary object\n",
    "    '''\n",
    "    doc_bow = []\n",
    "    tgt_set = set()\n",
    "\n",
    "    for src, tgt in tokenized_src_trg_pairs:\n",
    "        cur_bow = []\n",
    "        cur_bow.extend(src)\n",
    "        cur_bow.extend(tgt)\n",
    "        doc_bow.append(cur_bow)\n",
    "        \n",
    "    bow_dictionary = gensim.corpora.Dictionary(doc_bow)\n",
    "    # Remove single letter or character tokens\n",
    "    len_1_words = list(filter(lambda w: len(w) == 1, bow_dictionary.values()))\n",
    "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, len_1_words)))\n",
    "\n",
    "    def read_stopwords(fn):\n",
    "        return set([line.strip() for line in open(fn, encoding='utf-8') if len(line.strip()) != 0])\n",
    "\n",
    "    # Read stopwords from file (bow vocabulary should not contain stopwords)\n",
    "    STOPWORDS = gensim.parsing.preprocessing.STOPWORDS\n",
    "    #stopwords1 = read_stopwords(base_path+\"stopwords/stopwords.en.txt\")\n",
    "    #stopwords2 = read_stopwords(base_path+\"stopwords/stopwords.SE.txt\")\n",
    "    final_stopwords = set(STOPWORDS)#.union(stopwords1).union(stopwords2)\n",
    "\n",
    "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, final_stopwords)))\n",
    "\n",
    "    print(\"The original bow vocabulary: %d\" % len(bow_dictionary))\n",
    "    bow_dictionary.filter_extremes(no_below=3, keep_n=bow_vocab)\n",
    "    bow_dictionary.compactify()\n",
    "    bow_dictionary.id2token = dict([(id, t) for t, id in bow_dictionary.token2id.items()])\n",
    "    # for debug\n",
    "    sorted_dfs = sorted(bow_dictionary.dfs.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_dfs_token = [(bow_dictionary.id2token[id], cnt) for id, cnt in sorted_dfs]\n",
    "    print('The top 50 non-stop-words: ', sorted_dfs_token[:200])\n",
    "    return bow_dictionary\n",
    "\n",
    "# Build bag-of-word dictionary from training data\n",
    "print(\"Building bow dictionary from training data\")\n",
    "bow_dictionary = make_bow_dictionary(tokenized_train_pairs, base_path, bow_vocab)\n",
    "print(\"Bow dict_size: %d after filtered\" % len(bow_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict word2idx Saved\n",
      "Dict idx2word Saved\n",
      "Dict tag2idx Saved\n",
      "Dict idx2tag Saved\n",
      "Dict token_freq_counter Saved\n",
      "Dict bow_dictionary Saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(mydict, dict_name):\n",
    "    file_name=base_path+dict_name\n",
    "    f=open(file_name,'wb')\n",
    "    pickle.dump(mydict,f)\n",
    "    f.close()\n",
    "    print('Dict %s Saved'%dict_name)\n",
    "    \n",
    "save_dict(word2idx, 'word2idx')\n",
    "save_dict(idx2word, 'idx2word')\n",
    "save_dict(tag2idx, 'tag2idx')\n",
    "save_dict(idx2tag, 'idx2tag')\n",
    "save_dict(token_freq_counter, 'token_freq_counter')\n",
    "save_dict(bow_dictionary, 'bow_dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocab_size = 50000\n",
    "#tag_size = \n",
    "bow_vocab = 10000\n",
    "max_src_len = 200\n",
    "max_trg_len = 5\n",
    "\n",
    "def load_dict(dict_name):\n",
    "    file_name=base_path+dict_name\n",
    "    f=open(file_name,'rb')\n",
    "    mydict=pickle.load(f)\n",
    "    f.close()\n",
    "    return mydict\n",
    "\n",
    "bow_dictionary = load_dict('bow_dictionary')\n",
    "word2idx = load_dict('word2idx')\n",
    "tag2idx = load_dict('tag2idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_src_trg_files(tag=\"train\"):\n",
    "    '''\n",
    "    Read data according to the tag (train/valid/test), return a list of (src, trg) pairs\n",
    "    '''\n",
    "    if tag == \"train\":\n",
    "        src_file = train_src\n",
    "        trg_file = train_trg\n",
    "    elif tag == \"valid\":\n",
    "        src_file = valid_src\n",
    "        trg_file = valid_trg\n",
    "    else:\n",
    "        assert tag == \"test\"\n",
    "        src_file = test_src\n",
    "        trg_file = test_trg\n",
    "\n",
    "    tokenized_src = []\n",
    "    tokenized_trg = []\n",
    "    avg_post = []\n",
    "    avg_tag = []\n",
    "\n",
    "    for src_line, trg_line in zip(open(src_file, 'r'), open(trg_file, 'r')):\n",
    "        # process src and trg line\n",
    "        src_word_list = src_line.strip().split(' ')\n",
    "        trg_list = trg_line.strip().split(';')  # a list of target sequences\n",
    "\n",
    "        # Truncate the sequence if it is too long\n",
    "        avg_post.append(len(src_word_list))\n",
    "        avg_tag.append(len(trg_list))\n",
    "        src_word_list = src_word_list[:max_src_len]\n",
    "        trg_word_list = trg_list[:max_trg_len]\n",
    "\n",
    "        # Append the lines to the data\n",
    "        tokenized_src.append(src_word_list)\n",
    "        tokenized_trg.append(trg_word_list)\n",
    "\n",
    "    assert len(tokenized_src) == len(tokenized_trg), \\\n",
    "        'the number of records in source and target are not the same'\n",
    "\n",
    "    tokenized_pairs = list(zip(tokenized_src, tokenized_trg))\n",
    "    print(\"Finish reading %d lines of data from %s and %s\" % (len(tokenized_src), src_file, trg_file))\n",
    "    print('avg_post',np.mean(avg_post))\n",
    "    print('avg_tag',np.mean(avg_tag))\n",
    "    return tokenized_pairs\n",
    "\n",
    "def build_dataset(src_trgs_pairs, word2idx, tag2idx, bow_dictionary, tag=\"train\"):\n",
    "    '''\n",
    "    build train/valid/test dataset\n",
    "    '''\n",
    "    text = []\n",
    "    label = []\n",
    "    bow = [] \n",
    "    for idx, (source, targets) in enumerate(src_trgs_pairs):\n",
    "        src = [word2idx[w] if w in word2idx and word2idx[w] < vocab_size\n",
    "               else word2idx['<unk>'] for w in source]\n",
    "        trg = [tag2idx[w] for w in targets if w in tag2idx]\n",
    "        src_bow = bow_dictionary.doc2bow(source)\n",
    "        text.append(src)\n",
    "        label.append(trg)\n",
    "        bow.append(src_bow)\n",
    "        \n",
    "    bow = BowFeature(bow, bow_dictionary)\n",
    "    text = padding(text)\n",
    "    label =  [encode_one_hot(inst, len(tag2idx), label_from=0) for inst in label] \n",
    "    return np.array(text), np.array(label), np.array(bow)\n",
    "\n",
    "def padding(input_list):\n",
    "    input_list_lens = [len(l) for l in input_list]\n",
    "    max_seq_len = max(input_list_lens)\n",
    "    padded_batch = word2idx['<pad>'] * np.ones((len(input_list), max_seq_len), dtype=np.int)\n",
    "\n",
    "    for j in range(len(input_list)):\n",
    "        current_len = input_list_lens[j]\n",
    "        padded_batch[j][:current_len] = input_list[j]\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "def BowFeature(input_list, bow_dictionary):\n",
    "    '''\n",
    "    generate Bow Feature for train\\val\\test src\n",
    "    '''\n",
    "    bow_vocab = len(bow_dictionary)\n",
    "    res_src_bow = np.zeros((len(input_list), bow_vocab), dtype=np.int)\n",
    "    for idx, bow in enumerate(input_list):\n",
    "        bow_k = [k for k, v in bow]\n",
    "        bow_v = [v for k, v in bow]\n",
    "        res_src_bow[idx, bow_k] = bow_v\n",
    "    return res_src_bow\n",
    "\n",
    "def encode_one_hot(inst, vocab_size, label_from):\n",
    "    '''\n",
    "    one hot for a value x, int, x>=1\n",
    "    '''\n",
    "    one_hots = np.zeros(vocab_size, dtype=np.float32)\n",
    "    for value in inst:\n",
    "        one_hots[value-label_from]=1\n",
    "    return one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading 276056 lines of data from /data/pengyu/tag_rec/AU/train_src.txt and /data/pengyu/tag_rec/AU/train_trg.txt\n",
      "avg_post 94.1277059727012\n",
      "avg_tag 2.7585888370475558\n",
      "Finish reading 34507 lines of data from /data/pengyu/tag_rec/AU/valid_src.txt and /data/pengyu/tag_rec/AU/valid_trg.txt\n",
      "avg_post 93.17416756020518\n",
      "avg_tag 2.7572666415509897\n",
      "Finish reading 34507 lines of data from /data/pengyu/tag_rec/AU/test_src.txt and /data/pengyu/tag_rec/AU/test_trg.txt\n",
      "avg_post 93.779407076825\n",
      "avg_tag 2.75538296577506\n"
     ]
    }
   ],
   "source": [
    "# Build training set\n",
    "tokenized_train_pairs = read_src_trg_files(\"train\")\n",
    "train_text, train_label, train_bow = build_dataset(tokenized_train_pairs, word2idx, tag2idx, bow_dictionary,\"train\")\n",
    "\n",
    "tokenized_valid_pairs = read_src_trg_files('valid')\n",
    "valid_text, valid_label, valid_bow = build_dataset(tokenized_valid_pairs, word2idx, tag2idx, bow_dictionary,\"valid\")\n",
    "\n",
    "tokenized_test_pairs = read_src_trg_files('test')\n",
    "test_text, test_label, test_bow = build_dataset(tokenized_test_pairs, word2idx, tag2idx, bow_dictionary,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/pengyu/tag_rec/AU/processed_data/train_text.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/train_label.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/train_bow.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/valid_text.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/valid_label.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/valid_bow.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/test_text.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/test_label.npy Saved\n",
      "/data/pengyu/tag_rec/AU/processed_data/test_bow.npy Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_data(name):\n",
    "    data = eval(name)\n",
    "    \n",
    "    print('%s saved, shape:'%name, data.shape)\n",
    "    \n",
    "def save_data(data, name):\n",
    "    path = save_path+'/%s.npy'%name\n",
    "    np.save(path, data, allow_pickle=True) \n",
    "    print('%s Saved'%path)\n",
    "\n",
    "save_path = base_path+ 'processed_data'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "save_data(train_text, 'train_text')\n",
    "save_data(train_label, 'train_label')\n",
    "save_data(train_bow, 'train_bow')\n",
    "save_data(valid_text, 'valid_text')\n",
    "save_data(valid_label, 'valid_label')\n",
    "save_data(valid_bow, 'valid_bow')\n",
    "save_data(test_text, 'test_text')\n",
    "save_data(test_label, 'test_label')\n",
    "save_data(test_bow, 'test_bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW feature\n",
    "train_bow_data = data_utils.TensorDataset(torch.from_numpy(train_bow).type(torch.float32))\n",
    "val_bow_data = data_utils.TensorDataset(torch.from_numpy(valid_bow).type(torch.float32))                                          \n",
    "test_bow_data = data_utils.TensorDataset(torch.from_numpy(test_bow).type(torch.float32))\n",
    "\n",
    "train_bow_loader = data_utils.DataLoader(train_bow_data, batch_size, shuffle=True, drop_last=True)\n",
    "valid_bow_loader = data_utils.DataLoader(val_bow_data, batch_size, shuffle=True, drop_last=True)\n",
    "test_bow_loader = data_utils.DataLoader(test_bow_data, batch_size, drop_last=True)\n",
    "\n",
    "#Nomral feature and label\n",
    "train_data = data_utils.TensorDataset(torch.from_numpy(train_bow).type(torch.float32),\n",
    "                                      torch.from_numpy(train_text).type(torch.LongTensor),\n",
    "                                      torch.from_numpy(train_label).type(torch.LongTensor))\n",
    "val_data = data_utils.TensorDataset(torch.from_numpy(valid_bow).type(torch.float32),\n",
    "                                    torch.from_numpy(valid_text).type(torch.LongTensor),\n",
    "                                      torch.from_numpy(valid_label).type(torch.LongTensor))                                          \n",
    "test_data = data_utils.TensorDataset(torch.from_numpy(test_bow).type(torch.float32),\n",
    "                                     torch.from_numpy(test_text).type(torch.LongTensor),\n",
    "                                     torch.from_numpy(test_label).type(torch.LongTensor))\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size, drop_last=True)\n",
    "\n",
    "label_num = int(train_label.max())\n",
    "vocab_size = int(train_text.max())+2# +2 Don't Know Why\n",
    "fp('label_num')\n",
    "fp('vocab_size')\n",
    "print(\"load done\")\n",
    "\n",
    "return train_loader, val_loader, test_loader, label_num, vocab_size, train_bow_loader, \\\n",
    "valid_bow_loader, test_bow_loader, bow_dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
