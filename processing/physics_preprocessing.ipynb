{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML2DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert XML data to DataFrame Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#parameter\n",
    "dataset = 'physics'\n",
    "base_path = '/data/pengyu/tag_rec/%s/'%dataset\n",
    "data_path = '/data/pengyu/tag_rec/%s/Posts.xml'%dataset\n",
    "\n",
    "def xml2df(data_path):\n",
    "    '''\n",
    "    Input: XML data path\n",
    "    Output: Dataframe data that we need\n",
    "    '''\n",
    "    tree = ET.ElementTree(file=data_path)\n",
    "    root = tree.getroot()\n",
    "    body=[]\n",
    "    title=[]\n",
    "    tag=[]\n",
    "    userid=[]\n",
    "    for child_of_root in root:\n",
    "#         print(child_of_root.attrib)\n",
    "#         break\n",
    "        title.append(np.nan if child_of_root.get('Title')==None else child_of_root.attrib['Title'])\n",
    "        body.append(np.nan if child_of_root.get('Body')==None else child_of_root.attrib['Body'])\n",
    "        tag.append(np.nan if child_of_root.get('Tags')==None else child_of_root.attrib['Tags'])\n",
    "        userid.append(np.nan if child_of_root.get('OwnerUserId')==None else child_of_root.attrib['OwnerUserId'])\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "          'title':title,\n",
    "          'body':body,\n",
    "          'tag':tag,\n",
    "          'userid':userid\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = xml2df(data_path)\n",
    "df=df.dropna()\n",
    "df['body'] = df['title']+df['body']\n",
    "user_num = len(df['userid'].drop_duplicates())\n",
    "print('User Number：', user_num)\n",
    "df = df[['body','tag']]\n",
    "df = shuffle(df, random_state=42)\n",
    "df = df.iloc[:,:]\n",
    "print('original data Number：', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Html Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "wnl = WordNetLemmatizer() \n",
    "sw=stopwords.words('english')\n",
    "\n",
    "def src_remove_html(sentence):\n",
    "    temp = sentence\n",
    "    temp = re.sub(r'<[^>]+>', ' ',str(temp))#delete <*>\n",
    "    temp = re.sub(r'\\n', ' ',str(temp))#delete \\n\n",
    "    temp = re.sub(r'\\ \\d+\\ ', ' ',str(temp))#remove int number\n",
    "    temp = re.sub(r'\\ \\d+\\.\\d+\\ ', ' ',str(temp))#remove float number\n",
    "    temp = re.sub(r'\\s+[a-zA-Z]\\s+', ' ',str(temp))#remove singel letter  \n",
    "    temp = re.sub(r'[\\.,!?]\\s', ' ',str(temp))#remove , .for sentence \n",
    "    temp = re.sub(r'\\s[\\(\\[]|[\\)\\}]\\s', ' ',str(temp))#remove ()[]for sentence \n",
    "    temp = re.sub(r'\\:\\s', ' ',str(temp))#remove :for sentence \n",
    "    temp = re.sub(r'\\s+', ' ',str(temp))#merge many ' '\n",
    "    temp = temp.lower()\n",
    "    doc = temp.split()\n",
    "    temp = [wnl.lemmatize(word) for word in doc if word not in sw]\n",
    "    temp = ' '.join(temp)   \n",
    "    return temp\n",
    "\n",
    "def trg_remove_symbol(sentence):\n",
    "    temp = sentence\n",
    "    #temp = re.sub(r'[<>]', ' ',str(temp))\n",
    "    temp = re.sub(r'><', ';',str(temp))\n",
    "    temp = re.sub(r'[<>]', '',str(temp))\n",
    "    return temp    \n",
    "\n",
    "src = []\n",
    "trg = []\n",
    "\n",
    "for src_line, trg_line in df.values:\n",
    "    src_line = src_remove_html(src_line)    \n",
    "    trg_line = trg_remove_symbol(trg_line)\n",
    "    #delete Na row\n",
    "    if len(src_line)<2 or len(trg_line)<1:\n",
    "        print('=========================================')\n",
    "        print(src_line)\n",
    "        print(trg_line)\n",
    "        continue\n",
    "    src.append(src_line)\n",
    "    trg.append(trg_line)\n",
    "    \n",
    "assert len(src) == len(trg), \\\n",
    "    'the number of records in source and target are not the same'    \n",
    "print('data length is %d'%len(trg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(src)\n",
    "train_length = int(data_length*.8)\n",
    "valid_length = int(data_length*.9)\n",
    "train_src, valid_src, test_src = src[:train_length], src[train_length:valid_length],\\\n",
    "                                 src[valid_length:]\n",
    "train_trg, valid_trg, test_trg = trg[:train_length], trg[train_length:valid_length], \\\n",
    "                                 trg[valid_length:]\n",
    "\n",
    "# i = 177\n",
    "# print(df.iloc[i,0])\n",
    "# print(src[i])\n",
    "# print(tokenized_src[i])\n",
    "# print(trg[i])\n",
    "# print(tokenized_trg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, name):\n",
    "    with open(base_path+'%s.txt'%name, 'w') as f:\n",
    "        f.writelines(\"%s\\n\" % line for line in data)    \n",
    "\n",
    "save_data(train_src, 'train_src')\n",
    "save_data(valid_src, 'valid_src')\n",
    "save_data(test_src, 'test_src')\n",
    "save_data(train_trg, 'train_trg')\n",
    "save_data(valid_trg, 'valid_trg')\n",
    "save_data(test_trg, 'test_trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset = 'physics'\n",
    "vocab_size = 50000\n",
    "#tag_size = \n",
    "bow_vocab = 10000\n",
    "max_src_len = 200\n",
    "max_trg_len = 5\n",
    "\n",
    "base_path = '/data/pengyu/tag_rec/%s/'%dataset\n",
    "\n",
    "train_src = base_path + 'train_src.txt'\n",
    "train_trg = base_path + 'train_trg.txt'\n",
    "valid_src = base_path + 'valid_src.txt'\n",
    "valid_trg = base_path + 'valid_trg.txt'\n",
    "test_src = base_path + 'test_src.txt'\n",
    "test_trg = base_path + 'test_trg.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading 139085 lines of data from /data/pengyu/tag_rec/physics/train_src.txt and /data/pengyu/tag_rec/physics/train_trg.txt\n"
     ]
    }
   ],
   "source": [
    "def read_src_trg_files(tag=\"train\"):\n",
    "    '''\n",
    "    Read data according to the tag (train/valid/test), return a list of (src, trg) pairs\n",
    "    '''\n",
    "    if tag == \"train\":\n",
    "        src_file = train_src\n",
    "        trg_file = train_trg\n",
    "    elif tag == \"valid\":\n",
    "        src_file = valid_src\n",
    "        trg_file = valid_trg\n",
    "    else:\n",
    "        assert tag == \"test\"\n",
    "        src_file = test_src\n",
    "        trg_file = test_trg\n",
    "\n",
    "    tokenized_src = []\n",
    "    tokenized_trg = []\n",
    "\n",
    "    for src_line, trg_line in zip(open(src_file, 'r'), open(trg_file, 'r')):\n",
    "        # process src and trg line\n",
    "        src_word_list = src_line.strip().split(' ')\n",
    "        trg_list = trg_line.strip().split(';')  # a list of target sequences\n",
    "        #trg_word_list = [trg.strip().split(' ') for trg in trg_list]\n",
    "\n",
    "        # Truncate the sequence if it is too long\n",
    "        src_word_list = src_word_list[:max_src_len]\n",
    "        trg_word_list = trg_list[:max_trg_len]\n",
    "\n",
    "        # Append the lines to the data\n",
    "        tokenized_src.append(src_word_list)\n",
    "        tokenized_trg.append(trg_word_list)\n",
    "\n",
    "    assert len(tokenized_src) == len(tokenized_trg), \\\n",
    "        'the number of records in source and target are not the same'\n",
    "\n",
    "    tokenized_pairs = list(zip(tokenized_src, tokenized_trg))\n",
    "    print(\"Finish reading %d lines of data from %s and %s\" % (len(tokenized_src), src_file, trg_file))\n",
    "    return tokenized_pairs\n",
    "\n",
    "# Tokenize training data, return a list of tuple, (src_word_list, [trg_1_word_list, trg_2_word_list, ...])\n",
    "tokenized_train_pairs = read_src_trg_files(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary from training data\n",
      "Total vocab_size: 600758, predefined vocab_size: 50000\n",
      "Total tag_size: 893\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_src_trg_pairs):\n",
    "    '''\n",
    "    Build the vocabulary from the training (src, trg) pairs\n",
    "    :param tokenized_src_trg_pairs: list of (src, trg) pairs\n",
    "    :return: word2idx, idx2word, token_freq_counter\n",
    "    '''\n",
    "    token_freq_counter = Counter()\n",
    "    token_freq_counter_tag = Counter()\n",
    "    for src_word_list, trg_word_lists in tokenized_src_trg_pairs:\n",
    "        token_freq_counter.update(src_word_list)\n",
    "        token_freq_counter_tag.update(trg_word_lists)\n",
    "\n",
    "    # Discard special tokens if already present\n",
    "    special_tokens = ['<pad>', '<unk>']\n",
    "    num_special_tokens = len(special_tokens)\n",
    "\n",
    "    for s_t in special_tokens:\n",
    "        if s_t in token_freq_counter:\n",
    "            del token_freq_counter[s_t]\n",
    "\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for idx, word in enumerate(special_tokens):\n",
    "        # '<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    sorted_word2idx = sorted(token_freq_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_words = [x[0] for x in sorted_word2idx]\n",
    "\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        word2idx[word] = idx + num_special_tokens\n",
    "\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        idx2word[idx + num_special_tokens] = word\n",
    "\n",
    "    tag2idx = dict()\n",
    "    idx2tag = dict()\n",
    "\n",
    "    sorted_tag2idx = sorted(token_freq_counter_tag.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_tags = [x[0] for x in sorted_tag2idx]\n",
    "\n",
    "    for idx, tag in enumerate(sorted_tags):\n",
    "        tag2idx[tag] = idx\n",
    "\n",
    "    for idx, tag in enumerate(sorted_tags):\n",
    "        idx2tag[idx] = tag        \n",
    "             \n",
    "    return word2idx, idx2word, token_freq_counter, tag2idx, idx2tag\n",
    "\n",
    "# Build vocabulary from training src and trg\n",
    "print(\"Building vocabulary from training data\")\n",
    "word2idx, idx2word, token_freq_counter, tag2idx, idx2tag = build_vocab(tokenized_train_pairs)\n",
    "print(\"Total vocab_size: %d, predefined vocab_size: %d\" % (len(word2idx), vocab_size))\n",
    "print(\"Total tag_size: %d\" %len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bow dictionary from training data\n",
      "The original bow vocabulary: 600653\n",
      "The top 50 non-stop-words:  [('question', 37446), ('know', 29736), ('like', 25994), ('energy', 24199), ('time', 24129), (\"i'm\", 22850), ('field', 22588), ('equation', 22317), ('understand', 20789), ('point', 19024), ('mean', 17707), ('given', 17707), ('quantum-mechanics', 17470), ('particle', 17378), ('force', 17363), ('way', 17356), ('case', 16653), ('state', 16480), ('mass', 16215), ('example', 16112), ('problem', 15481), ('homework-and-exercises', 15108), ('different', 15105), ('use', 14224), ('possible', 14176), ('answer', 13681), ('quantum', 13621), ('following', 13397), ('space', 13103), ('constant', 13006), ('$$', 12993), ('theory', 12900), ('light', 12879), ('think', 12760), ('physic', 12752), ('term', 12595), ('function', 12540), ('velocity', 12334), ('work', 12280), ('change', 12256), ('newtonian-mechanics', 11728), ('need', 11199), ('consider', 10968), ('correct', 10926), ('electromagnetism', 10827), ('wave', 10742), ('right', 10484), ('object', 10449), ('direction', 10402), ('second', 10346), ('form', 10325), ('want', 10323), ('electron', 10289), ('speed', 10191), ('potential', 9969), ('result', 9937), ('value', 9881), ('help', 9866), (\"i've\", 9468), ('charge', 9433), ('law', 9360), ('momentum', 9320), ('number', 9258), ('come', 9251), ('calculate', 9223), ('distance', 9188), (\"can't\", 9025), ('trying', 8952), ('book', 8804), ('explain', 8802), ('thermodynamics', 8710), ('surface', 8690), ('general', 8591), ('equal', 8562), ('gravity', 8528), ('zero', 8509), ('wrong', 8353), ('difference', 8286), ('motion', 8247), ('assume', 8139), ('thing', 8119), ('vector', 8080), ('effect', 7979), ('current', 7967), ('electric', 7941), ('mechanic', 7933), ('density', 7907), ('moving', 7872), ('quantum-field-theory', 7761), ('temperature', 7761), ('read', 7743), ('general-relativity', 7639), ('solution', 7461), ('suppose', 7430), ('sure', 7365), ('order', 7319), ('let', 7272), ('physical', 7245), ('understanding', 7227), ('magnetic', 7166), ('i.e', 7163), ('look', 7162), ('simple', 6957), ('far', 6930), ('actually', 6857), ('matter', 6852), ('formula', 6845), ('operator', 6819), ('thought', 6696), ('model', 6668), ('sense', 6666), ('photon', 6610), ('reference', 6576), ('fact', 6549), ('source', 6505), ('small', 6492), ('true', 6441), ('position', 6413), ('experiment', 6402), ('line', 6294), ('reading', 6261), ('special-relativity', 6216), ('according', 6184), ('thanks', 6142), ('definition', 6121), ('able', 6107), ('defined', 6075), ('reason', 6075), ('explanation', 6004), ('body', 5990), ('idea', 5959), ('acceleration', 5957), ('end', 5910), ('total', 5885), ('pressure', 5853), ('length', 5846), ('inside', 5817), ('relativity', 5777), ('condition', 5777), ('looking', 5775), ('exactly', 5734), ('gravitational', 5695), ('going', 5638), ('related', 5593), ('water', 5570), ('relation', 5549), ('start', 5529), ('free', 5517), ('paper', 5495), ('increase', 5459), ('earth', 5442), ('set', 5395), ('similar', 5331), ('instead', 5261), ('high', 5257), ('note', 5208), ('frequency', 5152), ('atom', 5148), ('forces', 5138), ('universe', 5129), ('measure', 5123), ('coordinate', 5104), ('classical', 5087), ('heat', 5040), (\"let's\", 5025), ('cause', 5023), ('principle', 5016), ('said', 5016), ('classical-mechanics', 5015), ('process', 5013), ('confused', 4984), ('air', 4979), ('level', 4973), ('symmetry', 4927), ('happens', 4926), ('seen', 4897), ('angle', 4889), ('material', 4883), ('spacetime', 4837), ('simply', 4824), ('spin', 4814), ('kind', 4799), ('solve', 4796), ('component', 4783), ('assuming', 4771), ('angular', 4766), ('real', 4761), ('edit', 4761), ('certain', 4750), ('single', 4732), ('hamiltonian', 4730), ('hole', 4730), ('frame', 4703), ('wondering', 4700), ('le', 4687), ('calculation', 4663), ('course', 4660), ('radius', 4648), ('integral', 4602), ('particular', 4564)]\n",
      "Bow dict_size: 10000 after filtered\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def make_bow_dictionary(tokenized_src_trg_pairs, base_path, bow_vocab):\n",
    "    '''\n",
    "    Build bag-of-word dictionary from tokenized_src_trg_pairs\n",
    "    :param tokenized_src_trg_pairs: a list of (src, trg) pairs\n",
    "    :param data_dir: data address, for distinguishing Weibo/Twitter/StackExchange\n",
    "    :param bow_vocab: the size the bow vocabulary\n",
    "    :return: bow_dictionary, a gensim.corpora.Dictionary object\n",
    "    '''\n",
    "    doc_bow = []\n",
    "    tgt_set = set()\n",
    "\n",
    "    for src, tgt in tokenized_src_trg_pairs:\n",
    "        cur_bow = []\n",
    "        cur_bow.extend(src)\n",
    "        cur_bow.extend(tgt)\n",
    "        doc_bow.append(cur_bow)\n",
    "        \n",
    "    bow_dictionary = gensim.corpora.Dictionary(doc_bow)\n",
    "    # Remove single letter or character tokens\n",
    "    len_1_words = list(filter(lambda w: len(w) == 1, bow_dictionary.values()))\n",
    "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, len_1_words)))\n",
    "\n",
    "    def read_stopwords(fn):\n",
    "        return set([line.strip() for line in open(fn, encoding='utf-8') if len(line.strip()) != 0])\n",
    "\n",
    "    # Read stopwords from file (bow vocabulary should not contain stopwords)\n",
    "    STOPWORDS = gensim.parsing.preprocessing.STOPWORDS\n",
    "    #stopwords1 = read_stopwords(base_path+\"stopwords/stopwords.en.txt\")\n",
    "    #stopwords2 = read_stopwords(base_path+\"stopwords/stopwords.SE.txt\")\n",
    "    final_stopwords = set(STOPWORDS)#.union(stopwords1).union(stopwords2)\n",
    "\n",
    "    bow_dictionary.filter_tokens(list(map(bow_dictionary.token2id.get, final_stopwords)))\n",
    "\n",
    "    print(\"The original bow vocabulary: %d\" % len(bow_dictionary))\n",
    "    bow_dictionary.filter_extremes(no_below=3, keep_n=bow_vocab)\n",
    "    bow_dictionary.compactify()\n",
    "    bow_dictionary.id2token = dict([(id, t) for t, id in bow_dictionary.token2id.items()])\n",
    "    # for debug\n",
    "    sorted_dfs = sorted(bow_dictionary.dfs.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_dfs_token = [(bow_dictionary.id2token[id], cnt) for id, cnt in sorted_dfs]\n",
    "    print('The top 50 non-stop-words: ', sorted_dfs_token[:200])\n",
    "    return bow_dictionary\n",
    "\n",
    "# Build bag-of-word dictionary from training data\n",
    "print(\"Building bow dictionary from training data\")\n",
    "bow_dictionary = make_bow_dictionary(tokenized_train_pairs, base_path, bow_vocab)\n",
    "print(\"Bow dict_size: %d after filtered\" % len(bow_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict word2idx Saved\n",
      "Dict idx2word Saved\n",
      "Dict tag2idx Saved\n",
      "Dict idx2tag Saved\n",
      "Dict token_freq_counter Saved\n",
      "Dict bow_dictionary Saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(mydict, dict_name):\n",
    "    file_name=base_path+dict_name\n",
    "    f=open(file_name,'wb')\n",
    "    pickle.dump(mydict,f)\n",
    "    f.close()\n",
    "    print('Dict %s Saved'%dict_name)\n",
    "    \n",
    "save_dict(word2idx, 'word2idx')\n",
    "save_dict(idx2word, 'idx2word')\n",
    "save_dict(tag2idx, 'tag2idx')\n",
    "save_dict(idx2tag, 'idx2tag')\n",
    "save_dict(token_freq_counter, 'token_freq_counter')\n",
    "save_dict(bow_dictionary, 'bow_dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vocab_size = 50000\n",
    "#tag_size = \n",
    "bow_vocab = 10000\n",
    "max_src_len = 200\n",
    "max_trg_len = 5\n",
    "\n",
    "def load_dict(dict_name):\n",
    "    file_name=base_path+dict_name\n",
    "    f=open(file_name,'rb')\n",
    "    mydict=pickle.load(f)\n",
    "    f.close()\n",
    "    return mydict\n",
    "\n",
    "bow_dictionary = load_dict('bow_dictionary')\n",
    "word2idx = load_dict('word2idx')\n",
    "tag2idx = load_dict('tag2idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_src_trg_files(tag=\"train\"):\n",
    "    '''\n",
    "    Read data according to the tag (train/valid/test), return a list of (src, trg) pairs\n",
    "    '''\n",
    "    if tag == \"train\":\n",
    "        src_file = train_src\n",
    "        trg_file = train_trg\n",
    "    elif tag == \"valid\":\n",
    "        src_file = valid_src\n",
    "        trg_file = valid_trg\n",
    "    else:\n",
    "        assert tag == \"test\"\n",
    "        src_file = test_src\n",
    "        trg_file = test_trg\n",
    "\n",
    "    tokenized_src = []\n",
    "    tokenized_trg = []\n",
    "    avg_post = []\n",
    "    avg_tag = []\n",
    "\n",
    "    for src_line, trg_line in zip(open(src_file, 'r'), open(trg_file, 'r')):\n",
    "        # process src and trg line\n",
    "        src_word_list = src_line.strip().split(' ')\n",
    "        trg_list = trg_line.strip().split(';')  # a list of target sequences\n",
    "\n",
    "        # Truncate the sequence if it is too long\n",
    "        avg_post.append(len(src_word_list))\n",
    "        avg_tag.append(len(trg_list))\n",
    "        src_word_list = src_word_list[:max_src_len]\n",
    "        trg_word_list = trg_list[:max_trg_len]\n",
    "\n",
    "        # Append the lines to the data\n",
    "        tokenized_src.append(src_word_list)\n",
    "        tokenized_trg.append(trg_word_list)\n",
    "\n",
    "    assert len(tokenized_src) == len(tokenized_trg), \\\n",
    "        'the number of records in source and target are not the same'\n",
    "\n",
    "    tokenized_pairs = list(zip(tokenized_src, tokenized_trg))\n",
    "    print(\"Finish reading %d lines of data from %s and %s\" % (len(tokenized_src), src_file, trg_file))\n",
    "    print('avg_post',np.mean(avg_post))\n",
    "    print('avg_tag',np.mean(avg_tag))\n",
    "    return tokenized_pairs\n",
    "\n",
    "def build_dataset(src_trgs_pairs, word2idx, tag2idx, bow_dictionary, tag=\"train\"):\n",
    "    '''\n",
    "    build train/valid/test dataset\n",
    "    '''\n",
    "    text = []\n",
    "    label = []\n",
    "    bow = [] \n",
    "    for idx, (source, targets) in enumerate(src_trgs_pairs):\n",
    "        src = [word2idx[w] if w in word2idx and word2idx[w] < vocab_size\n",
    "               else word2idx['<unk>'] for w in source]\n",
    "        trg = [tag2idx[w] for w in targets if w in tag2idx]\n",
    "        src_bow = bow_dictionary.doc2bow(source)\n",
    "        text.append(src)\n",
    "        label.append(trg)\n",
    "        bow.append(src_bow)\n",
    "        \n",
    "    bow = BowFeature(bow, bow_dictionary)\n",
    "    text = padding(text)\n",
    "    label =  [encode_one_hot(inst, len(tag2idx), label_from=0) for inst in label] \n",
    "    return np.array(text), np.array(label), np.array(bow)\n",
    "\n",
    "def padding(input_list):\n",
    "    input_list_lens = [len(l) for l in input_list]\n",
    "    max_seq_len = max(input_list_lens)\n",
    "    padded_batch = word2idx['<pad>'] * np.ones((len(input_list), max_seq_len), dtype=np.int)\n",
    "\n",
    "    for j in range(len(input_list)):\n",
    "        current_len = input_list_lens[j]\n",
    "        padded_batch[j][:current_len] = input_list[j]\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "def BowFeature(input_list, bow_dictionary):\n",
    "    '''\n",
    "    generate Bow Feature for train\\val\\test src\n",
    "    '''\n",
    "    bow_vocab = len(bow_dictionary)\n",
    "    res_src_bow = np.zeros((len(input_list), bow_vocab), dtype=np.int)\n",
    "    for idx, bow in enumerate(input_list):\n",
    "        bow_k = [k for k, v in bow]\n",
    "        bow_v = [v for k, v in bow]\n",
    "        res_src_bow[idx, bow_k] = bow_v\n",
    "    return res_src_bow\n",
    "\n",
    "def encode_one_hot(inst, vocab_size, label_from):\n",
    "    '''\n",
    "    one hot for a value x, int, x>=1\n",
    "    '''\n",
    "    one_hots = np.zeros(vocab_size, dtype=np.float32)\n",
    "    for value in inst:\n",
    "        one_hots[value-label_from]=1\n",
    "    return one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading 139085 lines of data from /data/pengyu/tag_rec/physics/train_src.txt and /data/pengyu/tag_rec/physics/train_trg.txt\n",
      "avg_post 75.54165438400977\n",
      "avg_tag 3.1780206348635724\n",
      "Finish reading 17386 lines of data from /data/pengyu/tag_rec/physics/valid_src.txt and /data/pengyu/tag_rec/physics/valid_trg.txt\n",
      "avg_post 75.41107787875302\n",
      "avg_tag 3.176176233751294\n",
      "Finish reading 17386 lines of data from /data/pengyu/tag_rec/physics/test_src.txt and /data/pengyu/tag_rec/physics/test_trg.txt\n",
      "avg_post 76.05734499022202\n",
      "avg_tag 3.156102611296445\n"
     ]
    }
   ],
   "source": [
    "# Build training set\n",
    "tokenized_train_pairs = read_src_trg_files(\"train\")\n",
    "train_text, train_label, train_bow = build_dataset(tokenized_train_pairs, word2idx, tag2idx, bow_dictionary,\"train\")\n",
    "\n",
    "tokenized_valid_pairs = read_src_trg_files('valid')\n",
    "valid_text, valid_label, valid_bow = build_dataset(tokenized_valid_pairs, word2idx, tag2idx, bow_dictionary,\"valid\")\n",
    "\n",
    "tokenized_test_pairs = read_src_trg_files('test')\n",
    "test_text, test_label, test_bow = build_dataset(tokenized_test_pairs, word2idx, tag2idx, bow_dictionary,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/pengyu/tag_rec/physics/processed_data/train_text.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/train_label.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/train_bow.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/valid_text.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/valid_label.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/valid_bow.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/test_text.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/test_label.npy Saved\n",
      "/data/pengyu/tag_rec/physics/processed_data/test_bow.npy Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_data(name):\n",
    "    data = eval(name)\n",
    "    \n",
    "    print('%s saved, shape:'%name, data.shape)\n",
    "    \n",
    "def save_data(data, name):\n",
    "    path = save_path+'/%s.npy'%name\n",
    "    np.save(path, data, allow_pickle=True) \n",
    "    print('%s Saved'%path)\n",
    "\n",
    "save_path = base_path+ 'processed_data'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "save_data(train_text, 'train_text')\n",
    "save_data(train_label, 'train_label')\n",
    "save_data(train_bow, 'train_bow')\n",
    "save_data(valid_text, 'valid_text')\n",
    "save_data(valid_label, 'valid_label')\n",
    "save_data(valid_bow, 'valid_bow')\n",
    "save_data(test_text, 'test_text')\n",
    "save_data(test_label, 'test_label')\n",
    "save_data(test_bow, 'test_bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW feature\n",
    "train_bow_data = data_utils.TensorDataset(torch.from_numpy(train_bow).type(torch.float32))\n",
    "val_bow_data = data_utils.TensorDataset(torch.from_numpy(valid_bow).type(torch.float32))                                          \n",
    "test_bow_data = data_utils.TensorDataset(torch.from_numpy(test_bow).type(torch.float32))\n",
    "\n",
    "train_bow_loader = data_utils.DataLoader(train_bow_data, batch_size, shuffle=True, drop_last=True)\n",
    "valid_bow_loader = data_utils.DataLoader(val_bow_data, batch_size, shuffle=True, drop_last=True)\n",
    "test_bow_loader = data_utils.DataLoader(test_bow_data, batch_size, drop_last=True)\n",
    "\n",
    "#Nomral feature and label\n",
    "train_data = data_utils.TensorDataset(torch.from_numpy(train_bow).type(torch.float32),\n",
    "                                      torch.from_numpy(train_text).type(torch.LongTensor),\n",
    "                                      torch.from_numpy(train_label).type(torch.LongTensor))\n",
    "val_data = data_utils.TensorDataset(torch.from_numpy(valid_bow).type(torch.float32),\n",
    "                                    torch.from_numpy(valid_text).type(torch.LongTensor),\n",
    "                                      torch.from_numpy(valid_label).type(torch.LongTensor))                                          \n",
    "test_data = data_utils.TensorDataset(torch.from_numpy(test_bow).type(torch.float32),\n",
    "                                     torch.from_numpy(test_text).type(torch.LongTensor),\n",
    "                                     torch.from_numpy(test_label).type(torch.LongTensor))\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size, drop_last=True)\n",
    "\n",
    "label_num = int(train_label.max())\n",
    "vocab_size = int(train_text.max())+2# +2 Don't Know Why\n",
    "fp('label_num')\n",
    "fp('vocab_size')\n",
    "print(\"load done\")\n",
    "\n",
    "return train_loader, val_loader, test_loader, label_num, vocab_size, train_bow_loader, \\\n",
    "valid_bow_loader, test_bow_loader, bow_dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
